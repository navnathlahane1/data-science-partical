{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Navnath Lahane \n",
    "\n",
    "Roll Noll : 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "67ZO0C_9u9Bb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk   # Natural Language Tool Kit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLZY-teQvpBf",
    "outputId": "47d9ad5a-a69e-4702-fec7-e2a63b4518cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')    # punkt is the required package for tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVFrsrX9wI0q",
    "outputId": "d1b56c9d-d0f7-452a-c8c0-613988ef8bf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '!', ',', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n",
      "\n",
      "['Hello everyone!, Welcome to my blog post on Medium.', 'We are studying Natural Language Processing.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "sent = \"Hello everyone!, Welcome to my blog post on Medium. We are studying Natural Language Processing.\"\n",
    "print(word_tokenize(sent))\n",
    "print()\n",
    "print(sent_tokenize(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rSR-itZuwsZ9",
    "outputId": "766f98fc-efcc-4063-c52b-88d03a854e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYMocpELw8AF",
    "outputId": "f12f7613-cb6f-462f-ea30-02fb708c3a04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the unclean version: ['Hello', 'everyone', '!', ',', 'Welcome', 'to', 'my', 'blog', 'post', 'on', 'Medium', '.', 'We', 'are', 'studying', 'Natural', 'Language', 'Processing', '.']\n",
      "This is the cleaned version: ['Hello', 'everyone', '!', ',', 'Welcome', 'blog', 'post', 'Medium', '.', 'We', 'studying', 'Natural', 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords        # the corpus module is an extremely useful one. \n",
    "                                         \n",
    "stop_words = stopwords.words('english')  # this is the full list of all stop-words stored in nltk\n",
    "\n",
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "print(\"This is the unclean version:\", token)\n",
    "print(\"This is the cleaned version:\", cleaned_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1QEI9zVxOUd",
    "outputId": "94d05d7b-6913-43ef-caa1-3f46862f1f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'demo', 'text', 'for', 'nlp', 'use', 'nltk', '.', 'full', 'form', 'of', 'nltk', 'is', 'natur', 'languag', 'toolkit']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "text=\"This is a Demo Text for NLP using NLTK. Full form of NLTK is Natural Language Toolkit\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "stemmed_word = [snowball_stemmer.stem(word) for word in word_tokens]\n",
    "print (stemmed_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HR5UsC2vxRbg",
    "outputId": "ab951fb5-165a-44cb-a74e-a008eb6f659d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i967nDafxUAV",
    "outputId": "a616eab9-2dca-4f41-8cc2-78846da5754e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'dog', 'are', 'barking', 'outside', '.', 'Are', 'the', 'cat', 'in', 'the', 'garden', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#is based on The Porter Stemming Algorithm\n",
    "stopword = stopwords.words('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"the dogs are barking outside. Are the cats in the garden?\"\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "print (lemmatized_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhfIhs73xWfC",
    "outputId": "84f3d5a3-430e-4f4a-aebf-6cfdef49fea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2HldHO5xZAq",
    "outputId": "a3f1dc7d-fb97-4b12-d29f-45b24de0fde7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG'), ('outside', 'IN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"the dogs are barking outside.\"\n",
    "word = nltk.word_tokenize(text)\n",
    "pos_tag = nltk.pos_tag(word)\n",
    "print (pos_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "75DzNulFxea2"
   },
   "outputs": [],
   "source": [
    "# 2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.\n",
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EqNvJX9jxg3w"
   },
   "outputs": [],
   "source": [
    "\n",
    "d0 = 'New York Times'\n",
    "d1 = 'New York Post'\n",
    "d2 = 'Los Angles Times'\n",
    "\n",
    "series = [d0, d1, d2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "W06SM0bqxjTU"
   },
   "outputs": [],
   "source": [
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xD_0-7SvxmX8",
    "outputId": "e4d43a87-1b2f-4bad-8301-93723d830666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'new': 2, 'york': 5, 'times': 4, 'post': 3, 'los': 1, 'angles': 0}\n",
      "\n",
      "tf-idf value:\n",
      "  (0, 4)\t0.5773502691896257\n",
      "  (0, 5)\t0.5773502691896257\n",
      "  (0, 2)\t0.5773502691896257\n",
      "  (1, 3)\t0.680918560398684\n",
      "  (1, 5)\t0.5178561161676974\n",
      "  (1, 2)\t0.5178561161676974\n",
      "  (2, 0)\t0.6227660078332259\n",
      "  (2, 1)\t0.6227660078332259\n",
      "  (2, 4)\t0.4736296010332684\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.         0.         0.57735027 0.         0.57735027 0.57735027]\n",
      " [0.         0.         0.51785612 0.68091856 0.         0.51785612]\n",
      " [0.62276601 0.62276601 0.         0.         0.4736296  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5Ook_NUx3YA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TE_B_06_Lab 7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
